{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42ce8018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnx\n",
      "  Downloading onnx-1.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/shizy/miniconda3/envs/vicuna/lib/python3.11/site-packages (from onnx) (1.24.3)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /home/shizy/miniconda3/envs/vicuna/lib/python3.11/site-packages (from onnx) (4.23.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /home/shizy/miniconda3/envs/vicuna/lib/python3.11/site-packages (from onnx) (4.6.2)\n",
      "Installing collected packages: onnx\n",
      "Successfully installed onnx-1.14.0\n",
      "Collecting onnxruntime\n",
      "  Downloading onnxruntime-1.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: coloredlogs in /home/shizy/miniconda3/envs/vicuna/lib/python3.11/site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /home/shizy/miniconda3/envs/vicuna/lib/python3.11/site-packages (from onnxruntime) (23.5.26)\n",
      "Requirement already satisfied: numpy>=1.24.2 in /home/shizy/miniconda3/envs/vicuna/lib/python3.11/site-packages (from onnxruntime) (1.24.3)\n",
      "Requirement already satisfied: packaging in /home/shizy/miniconda3/envs/vicuna/lib/python3.11/site-packages (from onnxruntime) (23.1)\n",
      "Requirement already satisfied: protobuf in /home/shizy/miniconda3/envs/vicuna/lib/python3.11/site-packages (from onnxruntime) (4.23.2)\n",
      "Requirement already satisfied: sympy in /home/shizy/miniconda3/envs/vicuna/lib/python3.11/site-packages (from onnxruntime) (1.12)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/shizy/miniconda3/envs/vicuna/lib/python3.11/site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/shizy/miniconda3/envs/vicuna/lib/python3.11/site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Installing collected packages: onnxruntime\n",
      "Successfully installed onnxruntime-1.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install onnx\n",
    "!pip install onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab6b2db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch: 2.0.1+cu117\n",
      "onnxruntime: 1.15.0\n",
      "onnx: 1.14.0\n",
      "transformers: 4.29.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import onnx\n",
    "import onnxruntime\n",
    "import transformers\n",
    "print(\"pytorch:\", torch.__version__)\n",
    "print(\"onnxruntime:\", onnxruntime.__version__)\n",
    "print(\"onnx:\", onnx.__version__)\n",
    "print(\"transformers:\", transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfec625",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "EXAMPLE_Text = [\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nList the advantages and disadvantages of solar energy.\\n\\n### Response:\", \n",
    "                \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nEdit this sentence so that the verb tense is consistent.\\n\\n### Input:\\nHe are eating a large slice of pizza.\\n\\n### Response:\"]\n",
    "model_name_or_path = \"/home/nfs_data/models/vicuna/vicuna-7b/vicuna_7b_weights_v1.1\"\n",
    "\n",
    "def get_tokenizer():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "    #tokenizer.padding_side = \"right\"\n",
    "    #tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def get_example_inputs(prompt_text=EXAMPLE_Text):\n",
    "    tokenizer = get_tokenizer(model_name_or_path)\n",
    "    encodings_dict = tokenizer.batch_encode_plus(prompt_text, padding=True)\n",
    "\n",
    "    input_ids = torch.tensor(encodings_dict[\"input_ids\"], dtype=torch.int32)\n",
    "    attention_mask = torch.tensor(encodings_dict[\"attention_mask\"], dtype=torch.int32)\n",
    "    position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "    position_ids.masked_fill_(position_ids < 0, 0)\n",
    "    position_ids = position_ids.to(torch.int32)\n",
    "\n",
    "    # Empty Past State for generating first word\n",
    "    empty_past = []\n",
    "    batch_size = input_ids.size(0)\n",
    "    sequence_length = input_ids.size(1)\n",
    "    past_shape = [2, batch_size, num_attention_heads, 0, hidden_size // num_attention_heads]\n",
    "    for i in range(num_layer):\n",
    "        empty_past.append(torch.empty(past_shape).type(torch.float32).to(device))\n",
    "\n",
    "    return input_ids, attention_mask, position_ids, empty_past\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaTokenizer, AutoModel\n",
    "\n",
    "def load_module(device=\"cuda\",num_gpus=1,debug=False):\n",
    "    torch_model = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=False)\n",
    "    if device == \"cuda\" and num_gpus == 1:\n",
    "        model.to(device)\n",
    "    if debug:\n",
    "        print(model)\n",
    "    return model\n",
    "    \n",
    "\n",
    "torch_model = load_module()\n",
    "torch_model.eval()\n",
    "\n",
    "input_ids, attention_mask, position_ids, empty_past = get_example_inputs()\n",
    "print(\"input_ids\", input_ids)\n",
    "print(\"attention_mask\", attention_mask)\n",
    "print(\"position_ids\", position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac26750",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate_stream(\n",
    "    model, tokenizer, params, device, context_len=2048, stream_interval=2\n",
    "):\n",
    "    prompt = params[\"prompt\"]\n",
    "    len_prompt = len(prompt)\n",
    "    temperature = float(params.get(\"temperature\", 1.0))\n",
    "    repetition_penalty = float(params.get(\"repetition_penalty\", 1.0))\n",
    "    top_p = float(params.get(\"top_p\", 1.0))\n",
    "    top_k = int(params.get(\"top_k\", -1))  # -1 means disable\n",
    "    max_new_tokens = int(params.get(\"max_new_tokens\", 256))\n",
    "    stop_str = params.get(\"stop\", None)\n",
    "    echo = bool(params.get(\"echo\", True))\n",
    "    stop_token_ids = params.get(\"stop_token_ids\", None) or []\n",
    "    stop_token_ids.append(tokenizer.eos_token_id)\n",
    "\n",
    "    logits_processor = prepare_logits_processor(\n",
    "        temperature, repetition_penalty, top_p, top_k\n",
    "    )\n",
    "\n",
    "    input_ids = tokenizer(prompt).input_ids\n",
    "    input_echo_len = len(input_ids)\n",
    "    output_ids = list(input_ids)\n",
    "\n",
    "    if model.config.is_encoder_decoder:\n",
    "        max_src_len = context_len\n",
    "    else:\n",
    "        max_src_len = context_len - max_new_tokens - 8\n",
    "\n",
    "    input_ids = input_ids[-max_src_len:]\n",
    "\n",
    "    if model.config.is_encoder_decoder:\n",
    "        encoder_output = model.encoder(\n",
    "            input_ids=torch.as_tensor([input_ids], device=device)\n",
    "        )[0]\n",
    "        start_ids = torch.as_tensor(\n",
    "            [[model.generation_config.decoder_start_token_id]],\n",
    "            dtype=torch.int64,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    past_key_values = out = None\n",
    "    for i in range(max_new_tokens):\n",
    "        if i == 0:\n",
    "            if model.config.is_encoder_decoder:\n",
    "                out = model.decoder(\n",
    "                    input_ids=start_ids,\n",
    "                    encoder_hidden_states=encoder_output,\n",
    "                    use_cache=True,\n",
    "                )\n",
    "                logits = model.lm_head(out[0])\n",
    "            else:\n",
    "                out = model(torch.as_tensor([input_ids], device=device), use_cache=True)\n",
    "                logits = out.logits\n",
    "            past_key_values = out.past_key_values\n",
    "        else:\n",
    "            if model.config.is_encoder_decoder:\n",
    "                out = model.decoder(\n",
    "                    input_ids=torch.as_tensor([[token]], device=device),\n",
    "                    encoder_hidden_states=encoder_output,\n",
    "                    use_cache=True,\n",
    "                    past_key_values=past_key_values,\n",
    "                )\n",
    "\n",
    "                logits = model.lm_head(out[0])\n",
    "            else:\n",
    "                out = model(\n",
    "                    input_ids=torch.as_tensor([[token]], device=device),\n",
    "                    use_cache=True,\n",
    "                    past_key_values=past_key_values,\n",
    "                )\n",
    "                logits = out.logits\n",
    "            past_key_values = out.past_key_values\n",
    "\n",
    "        if logits_processor:\n",
    "            if repetition_penalty > 1.0:\n",
    "                tmp_output_ids = torch.as_tensor([output_ids], device=logits.device)\n",
    "            else:\n",
    "                tmp_output_ids = None\n",
    "            last_token_logits = logits_processor(tmp_output_ids, logits[:, -1, :])[0]\n",
    "        else:\n",
    "            last_token_logits = logits[0, -1, :]\n",
    "\n",
    "        if device == \"mps\":\n",
    "            # Switch to CPU by avoiding some bugs in mps backend.\n",
    "            last_token_logits = last_token_logits.float().to(\"cpu\")\n",
    "\n",
    "        if temperature < 1e-5 or top_p < 1e-8:  # greedy\n",
    "            token = int(torch.argmax(last_token_logits))\n",
    "        else:\n",
    "            probs = torch.softmax(last_token_logits, dim=-1)\n",
    "            token = int(torch.multinomial(probs, num_samples=1))\n",
    "\n",
    "        output_ids.append(token)\n",
    "\n",
    "        if token in stop_token_ids:\n",
    "            stopped = True\n",
    "        else:\n",
    "            stopped = False\n",
    "\n",
    "        if i % stream_interval == 0 or i == max_new_tokens - 1 or stopped:\n",
    "            if echo:\n",
    "                tmp_output_ids = output_ids\n",
    "                rfind_start = len_prompt\n",
    "            else:\n",
    "                tmp_output_ids = output_ids[input_echo_len:]\n",
    "                rfind_start = 0\n",
    "\n",
    "            output = tokenizer.decode(\n",
    "                tmp_output_ids,\n",
    "                skip_special_tokens=True,\n",
    "                spaces_between_special_tokens=False,\n",
    "            )\n",
    "\n",
    "            partially_stopped = False\n",
    "            if stop_str:\n",
    "                if isinstance(stop_str, str):\n",
    "                    pos = output.rfind(stop_str, rfind_start)\n",
    "                    if pos != -1:\n",
    "                        output = output[:pos]\n",
    "                        stopped = True\n",
    "                    else:\n",
    "                        partially_stopped = partial_stop(output, stop_str)\n",
    "                elif isinstance(stop_str, Iterable):\n",
    "                    for each_stop in stop_str:\n",
    "                        pos = output.rfind(each_stop, rfind_start)\n",
    "                        if pos != -1:\n",
    "                            output = output[:pos]\n",
    "                            stopped = True\n",
    "                            break\n",
    "                        else:\n",
    "                            partially_stopped = partial_stop(output, each_stop)\n",
    "                            if partially_stopped:\n",
    "                                break\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid stop field type.\")\n",
    "\n",
    "            # prevent yielding partial stop sequence\n",
    "            if not partially_stopped:\n",
    "                yield {\n",
    "                    \"text\": output,\n",
    "                    \"usage\": {\n",
    "                        \"prompt_tokens\": input_echo_len,\n",
    "                        \"completion_tokens\": i,\n",
    "                        \"total_tokens\": input_echo_len + i,\n",
    "                    },\n",
    "                    \"finish_reason\": None,\n",
    "                }\n",
    "\n",
    "        if stopped:\n",
    "            break\n",
    "\n",
    "    # finish stream event, which contains finish reason\n",
    "    if i == max_new_tokens - 1:\n",
    "        finish_reason = \"length\"\n",
    "    elif stopped:\n",
    "        finish_reason = \"stop\"\n",
    "    else:\n",
    "        finish_reason = None\n",
    "\n",
    "    yield {\n",
    "        \"text\": output,\n",
    "        \"usage\": {\n",
    "            \"prompt_tokens\": input_echo_len,\n",
    "            \"completion_tokens\": i,\n",
    "            \"total_tokens\": input_echo_len + i,\n",
    "        },\n",
    "        \"finish_reason\": finish_reason,\n",
    "    }\n",
    "\n",
    "    # clean\n",
    "    del past_key_values, out\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
